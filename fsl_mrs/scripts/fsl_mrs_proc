#!/usr/bin/env python

# fsl_mrs_proc - script for individual MRS preprocessing stages
#
# Author:   Will Clarke <william.clarke@ndcn.ox.ac.uk>
#           Saad Jbabdi <saad@fmrib.ox.ac.uk>
#
# Copyright (C) 2020 University of Oxford 
# SHBASECOPYRIGHT

# Quick imports
#import argparse
import configargparse

from fsl_mrs import __version__
from fsl_mrs.utils.splash import splash


def main():
    # Parse command-line arguments
    p = configargparse.ArgParser(add_config_file_help=False,description="FSL Magnetic Resonance Spectroscopy - Preprocessing")

    p.add_argument('-v','--version', action='version', version=__version__)

    sp = p.add_subparsers(title='subcommands', description='Preprocessing subcommands')

    # Coil combination subcommand
    ccparser = sp.add_parser('coilcombine',help='Combine coils.')
    ccparser.add_argument('--file',help='Uncombined coil data file(s)',type=str, nargs='+',required=True)
    ccparser.add_argument('-r','--reference',help='Water unsuppressed reference data',type=str, nargs='+',required=False)
    ccparser.add_argument('--noprewhiten',help='Don''t prewhiten data before coil combination',action="store_false")
    ccparser.set_defaults(func=coilcombine)

    # Average subcommand
    avgparser = sp.add_parser('average',help='Average FIDs.')
    avgparser.add_argument('--file',help='MRS file(s)',type=str, nargs='+',required=True)
    avgparser.add_argument('--avgfiles',help='Average across files',action="store_true")
    avgparser.add_argument('--dim',help='Select dimension to average across',type=int)
    avgparser.set_defaults(func=average)

    #Align subcommand - frequency/phase alignment
    alignparser = sp.add_parser('align',help='Align FIDs.')
    alignparser.add_argument('--file',help='List of files to align',type=str, nargs='+',required=True)
    alignparser.add_argument('--ppm',help='ppm limits of alignment window (default=0.2->4.2)',type=float, nargs=2,metavar='<lower-limit upper-limit>',default=(0.2,4.2))
    alignparser.add_argument('--reference',help='Align to this reference data.',type=str,required=False)
    alignparser.set_defaults(func=align)

    # ECC subcommand - eddy current correction
    eccparser = sp.add_parser('ecc',help='Eddy current correction')     
    eccparser.add_argument('--file',help='Uncombined coil data file(s)',type=str, nargs='+',required=True)
    eccparser.add_argument('--reference',help='Phase reference data file(s)',type=str, nargs='+',required=True)
    eccparser.set_defaults(func=ecc)

    # remove subcommand - remove peak using HLSVD
    hlsvdparser = sp.add_parser('remove',help='Remove peak (default water) using HLSVD.') 
    hlsvdparser.add_argument('--file',help='Data file(s)',type=str, nargs='+',required=True)
    hlsvdparser.add_argument('--ppm',help='ppm limits of removal window',type=float, nargs=2,metavar='<lower-limit upper-limit>',default = [4.5,4.8])
    hlsvdparser.set_defaults(func=remove)

    # tshift subcommand - shift/resample in timedomain
    tshiftparser = sp.add_parser('tshift',help='shift/resample in timedomain.') 
    tshiftparser.add_argument('--file',help='Data file(s) to shift',type=str, nargs='+',required=True)
    tshiftparser.add_argument('--tshiftStart',help='Time shift at start (ms), negative pads with zeros, positive truncates',type=float,default = 0.0)
    tshiftparser.add_argument('--tshiftEnd',help='Time shift at end (ms), negative truncates, positive pads with zeros',type=float,default = 0.0)
    tshiftparser.add_argument('--samples',help='Resample to N points in FID.',type=int)
    tshiftparser.set_defaults(func=tshift)

    # truncate
    truncateparser = sp.add_parser('truncate',help='truncate by integer points in timedomain.') 
    truncateparser.add_argument('--file',help='Data file(s) to shift',type=str, nargs='+',required=True)
    truncateparser.add_argument('--points',help='Points to add/remove (+/-)',type=int,default = 0)
    truncateparser.add_argument('--pos',help=' ''first'' or ''last'' (default)',type=str,default = 'last')
    truncateparser.set_defaults(func=truncate)

    # apodize
    apodparser = sp.add_parser('apodize',help='Apodize FID.') 
    apodparser.add_argument('--file',help='Data file(s) to shift',type=str, nargs='+',required=True)
    apodparser.add_argument('--filter',help='Filter choice, either ''exp'' (default) or ''l2g''.',type=str,default = 'exp')
    apodparser.add_argument('--amount',help='Amount of broadening. In Hz for exp mode. Use space separated list for l2g.',type=float,nargs='+')
    apodparser.set_defaults(func=apodize)

    # fshift subcommand - shift in frequency domain
    fshiftparser = sp.add_parser('fshift',help='shift in frequency domain.') 
    fshiftparser.add_argument('--file',help='Data file(s) to shift',type=str, nargs='+',required=True)
    fshiftparser.add_argument('--shiftppm',help='Shift (ppm scale)',type=float)
    fshiftparser.add_argument('--shifthz',help='Shift (Hz scale)',type=float)
    fshiftparser.set_defaults(func=fshift)

    # unlike subcomand - find FIDs that are unlike
    unlikeparser = sp.add_parser('unlike',help='Identify unlike FIDs.') 
    unlikeparser.add_argument('--file',help='Data file(s) to shift',type=str, nargs='+',required=True)
    unlikeparser.add_argument('--sd',help='Exclusion limit (# of SD from mean,default=1.96)',type=float,default=1.96)
    unlikeparser.add_argument('--iter',help='Iterations of algorithm.',type=int,default=2)
    unlikeparser.add_argument('--ppm',help='ppm limits of alignment window',type=float, nargs=2,metavar='<lower-limit upper-limit>',default=None)
    unlikeparser.add_argument('--outputbad',help='Output failed FIDs',action="store_true")
    unlikeparser.set_defaults(func=unlike)

    # Phasing - based on maximum point in range
    phaseparser = sp.add_parser('phase',help='Phase spectrum based on maximum point in range') 
    phaseparser.add_argument('--file',help='Data file(s) to shift',type=str, nargs='+',required=True)
    phaseparser.add_argument('--ppm',help='ppm limits of alignment window',type=float, nargs=2,metavar='<lower-limit upper-limit>',default=(2.8,3.2))
    phaseparser.set_defaults(func=phase)

    # subtraction - subtraction of FIDs
    subtractparser = sp.add_parser('subtract',help='Subtract two FIDs') 
    subtractparser.add_argument('--file',help='Two data files to subtract (second from first)',type=str, nargs=2,required=True)
    subtractparser.set_defaults(func=subtract)

    # add - addition of FIDs
    subtractparser = sp.add_parser('add',help='Add two FIDs') 
    subtractparser.add_argument('--file',help='Two data files to add',type=str, nargs=2,required=True)
    subtractparser.set_defaults(func=add)

    # Arguments not assiciated with subcommands
    required     = p.add_argument_group('required arguments')
    optional     = p.add_argument_group('additional options')

    # REQUIRED ARGUMENTS 
    required.add_argument('--output',
                          required=True,type=str,metavar='<str>',
                          help='output folder')

    # ADDITONAL OPTIONAL ARGUMENTS    
    optional.add_argument('--overwrite',action="store_true",
                          help='overwrite existing output folder')
    optional.add_argument('--conjugate',action="store_true",
                          help='apply conjugate to FID')
    optional.add_argument('--filename',type=str,metavar='<str>',
                          help='Override output file name.')    
    optional.add_argument('--verbose',action="store_true",
                          help='spit out verbose info')
    optional.add('--config', required=False, is_config_file=True, help='configuration file')

    # Parse command-line arguments
    args = p.parse_args()
    
    # Output kickass splash screen
    if args.verbose:
        splash(logo='mrs')
    
    # Parse file arguments    
    datafiles,reffiles = parsefilearguments(args)

    # Handle data loading
    dataList = loadData(datafiles,refdatafiles=reffiles,conjugate=args.conjugate)
    # Call function - pass dict like view of args for compatibility with other modules
    dataout = args.func(dataList,vars(args))
    if isinstance(dataout,tuple):
        additionalOutputs = dataout[1:]
        dataout = dataout[0]
    else:
        additionalOutputs = None

    # Write data
    writeData(dataout,args)

    # Output any additional arguments
    if additionalOutputs is not None:
        print(additionalOutputs)
    
# ######################################################
# Imports
import os #time,sys, shutil, warnings
import numpy as np
from fsl_mrs.utils import preproc
from fsl_mrs.utils.mrs_io import check_datatype
from fsl_mrs.utils.mrs_io import fsl_io
# ######################################################

def parsefilearguments(args):
    # print(args.file)
    datafiles = args.file
    if 'reference' in args:
        # print(args.reference)
        reffiles = args.reference
    else:
        reffiles = None

    return datafiles,reffiles

# Data I/O functions
def loadData(datafiles,refdatafiles=None,conjugate=False):
    """ Load data from a list of data files.

    The data must be of nifti format and the spatial dimensions are preserved.
    Optionaly loads one or many reference files.
    Can conjugate.
    """

    # Do a check on the first file passed. The data must be of nifti type. 
    if check_datatype(datafiles[0]) != 'NIFTI':
        raise ValueError(f'Preprocessing routines only handle NIFTI format data. Please convert your data using spec2nii. {datafiles[0]} was identified as {check_datatype(datafiles[0])}.')
    datalist = []
    # Load data into datacontainer
    if refdatafiles and len(refdatafiles)==len(datafiles):
        for f,r in zip(datafiles,refdatafiles):
            data,header = fsl_io.readNIFTI(f,squeezeSVS=False)
            ref,refheader = fsl_io.readNIFTI(r,squeezeSVS=False)
            if conjugate:
                data = data.conj()
                ref = ref.conj()

            datalist.append(preproc.datacontainer(data,
                                                header,
                                                os.path.basename(f),
                                                reference=ref,
                                                refheader=refheader,
                                                reffilename=os.path.basename(r)))
    elif refdatafiles and len(refdatafiles)==1:
        ref,refheader = fsl_io.readNIFTI(refdatafiles[0],squeezeSVS=False)
        if conjugate:
            ref = ref.conj()

        for f in datafiles:
            data,header = fsl_io.readNIFTI(f,squeezeSVS=False)
            if conjugate:
                data = data.conj()
                
            datalist.append(preproc.datacontainer(data,
                                                header,
                                                os.path.basename(f),
                                                reference=ref,
                                                refheader=refheader,
                                                reffilename=os.path.basename(refdatafiles[0])))
    elif refdatafiles and len(refdatafiles)!=len(datafiles):
        raise ValueError('Pass one reference file, an equal number of references as data files or none.')
    else:
        for f in datafiles:
            data,header = fsl_io.readNIFTI(f,squeezeSVS=False)
            if conjugate:
                data = data.conj()
            datalist.append(preproc.datacontainer(data,header,os.path.basename(f)))
    return datalist

def writeData(dataobj,args):
    for iDx,d in enumerate(dataobj):
        if args.filename is None:
            fileout = os.path.join(args.output,d.datafilename)
        elif len(dataobj)==1:
            fileout = os.path.join(args.output,args.filename+'.nii.gz')
        else:
            fileout = os.path.join(args.output,args.filename+f'_{iDx:03.0f}.nii.gz')
        
        fsl_io.saveNIFTI(fileout,d.data,d.dataheader)

### Option functions ###
# Functions below here should be associated with a subcommand method specified above.
# They should call a method in preproc.py.

# Preprocessing functions    
def coilcombine(dataobj,args):
    dataout = []
    for d in dataobj:        
        combmain = np.zeros(d.data.shape[:4],dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):
            # If reference data is not loaded
            if d.reference is None:
                combmain[ijk] = preproc.combine_FIDs(d.data[ijk],'svd',do_prewhiten=~args['noprewhiten'])            
            else:
                # Otherwise                
                combW,refWeights = preproc.combine_FIDs(d.reference[ijk],'svd_weights',do_prewhiten=~args['noprewhiten'])
                combmain[ijk] = preproc.combine_FIDs(d.data[ijk],'weighted',weights=refWeights)

        # Reuse the file name as it's likely the coil uncombined data was all in one file anyway
        fileout = d.datafilename
        dataout.append(preproc.datacontainer(combmain,d.dataheader,fileout))

    return dataout

def average(dataobj,args):
    dataout = []
    if args['avgfiles']:        
        # Average data across all input files, outputing data the shape of any one file
        # combine_FIDs 'mean' takes mean across last dimension.
        allFileData = np.array([d.data for d in dataobj])
        meandata = preproc.combine_FIDs(allFileData.T,'mean').T
        dataout.append(preproc.datacontainer(meandata,dataobj[0].dataheader,dataobj[0].datafilename))

    elif args['dim']:
        for d in dataobj:
            # permute the selected dimension to the last
            meandata = preproc.combine_FIDs(np.moveaxis(d.data,args['dim'],-1),'mean')
            meandata = np.moveaxis(meandata,-1,args['dim'])
            dataout.append(preproc.datacontainer(meandata,d.dataheader,d.datafilename))
    else:
        raise RuntimeError('Neither avgfiles or dim arguments specified averaging dimension uncertain.')

    return dataout

def align(dataobj,args):

    bandwidth = dataobj[0].dataheader['bandwidth']
    centralFrequency = dataobj[0].dataheader['centralFrequency']*1E6
    allFIDs = [d.data for d in dataobj]
    for ijk in np.ndindex(dataobj[0].data.shape[:3]):    
        FIDlist = [fid[ijk] for fid in allFIDs]        

        tmpFIDS = preproc.phase_freq_align(FIDlist,bandwidth,centralFrequency,ppmlim=args['ppm'],niter=2,verbose=False,target=dataobj[0].reference)

        for iDx,fid in enumerate(tmpFIDS):
            allFIDs[iDx][ijk,...] = fid
        
    dataout = []
    for f,d in zip(allFIDs,dataobj):
        dataout.append(preproc.datacontainer(f,d.dataheader,d.datafilename))

    return dataout


def ecc(dataobj,args):
    dataout = []
    for d in dataobj:
        corrected = np.zeros(d.data.shape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):
            corrected[ijk] = preproc.eddy_correct(d.data[ijk],d.reference[ijk])
        dataout.append(preproc.datacontainer(corrected,d.dataheader,d.datafilename))
    return dataout

def remove(dataobj,args):
    dataout = []
    for d in dataobj:
        freqlimits = (np.array(args['ppm'])-4.65)*d.dataheader['centralFrequency']
        removed = np.zeros(d.data.shape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):
            removed[ijk] = preproc.hlsvd(d.data[ijk],d.dataheader['dwelltime'],freqlimits)

        dataout.append(preproc.datacontainer(removed,d.dataheader,d.datafilename))
    return dataout

def tshift(dataobj,args):
    dataout = []
    for d in dataobj:
        shifted = np.zeros(d.data.shape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):
            shifted[ijk],newDT = preproc.timeshift(d.data[ijk],
                                        d.dataheader['dwelltime'],
                                        args['tshiftStart'],
                                        args['tshiftEnd'],
                                        args['samples'])

        d.dataheader['dwelltime'] = newDT
        d.dataheader['bandwidth'] = 1/newDT
        dataout.append(preproc.datacontainer(shifted,d.dataheader,d.datafilename))

    return dataout

def truncate(dataobj,args):
    dataout = []
    for d in dataobj:
        baseShape = list(d.data.shape)        
        baseShape[3] += args['points']
        newFID = np.zeros(baseShape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):
            if args['points']>0:
                newFID[ijk] = preproc.pad(d.data[ijk],np.abs(args['points']),args['pos'])
            elif args['points']<0:
                newFID[ijk] = preproc.truncate(d.data[ijk],np.abs(args['points']),args['pos'])
            else:
                newFID[ijk] = d.data[ijk]
        
        dataout.append(preproc.datacontainer(newFID,d.dataheader,d.datafilename))
    return dataout

def apodize(dataobj,args):
    dataout = []
    for d in dataobj:
        broadended = np.zeros(d.data.shape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):
            broadended[ijk] = preproc.apodize(d.data[ijk],
                                d.dataheader['dwelltime'],
                                args['amount'],
                                filter= args['filter'])
        dataout.append(preproc.datacontainer(broadended,d.dataheader,d.datafilename))

    return dataout


def fshift(dataobj,args):
    dataout = []
    for d in dataobj:
        if args['shiftppm'] is not None:            
            shift = args['shiftppm']*d.dataheader['centralFrequency']
        elif args['shifthz'] is not None:            
            shift = args['shifthz']
        else:
            raise ValueError('Specify --shiftppm or --shifthz.')
        shifted = np.zeros(d.data.shape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):        
            shifted[ijk] = preproc.timeshift(d.data[ijk],
                                        d.dataheader['dwelltime'],
                                        shift)
        dataout.append(preproc.datacontainer(shifted,d.dataheader,d.datafilename))

    return dataout

def unlike(dataobj,args):
    if dataobj[0].data.shape[:3] != (1,1,1):
        raise ValueError('unlike subcommand only works on single voxel data. It is unclear what should happen with MRSI data.')

    FIDlist = [d.data[0,0,0,...] for d in dataobj] # Here we are assuming only SVS data.
    bandwidth = dataobj[0].dataheader['bandwidth']
    centralFrequency = dataobj[0].dataheader['centralFrequency']

    goodFIDs,badFIDs,gIndicies,bIndicies,metric = preproc.identifyUnlikeFIDs(FIDlist,bandwidth,centralFrequency,ppmlim=args['ppm'],sdlimit = args['sd'],iterations=args['iter'],shift=True)

    dataout = []
    for f,iDx in zip(goodFIDs,gIndicies):
        dataout.append(preproc.datacontainer(f,dataobj[iDx].dataheader,dataobj[iDx].datafilename))

    if args['outputbad']:
        for f,iDx in zip(badFIDs,bIndicies):
            dataout.append(preproc.datacontainer(f,dataobj[iDx].dataheader,dataobj[iDx].datafilename+'_FAIL'))

    return dataout

def phase(dataobj,args):
    dataout = []
    for d in dataobj:
        phased = np.zeros(d.data.shape,dtype=np.complex128)
        for ijk in np.ndindex(d.data.shape[:3]):    
            phased[ijk] = preproc.phaseCorrect(d.data[ijk],
                                d.dataheader['bandwidth'],
                                d.dataheader['centralFrequency']*1E6,
                                ppmlim = args['ppm'])
        dataout.append(preproc.datacontainer(phased,d.dataheader,d.datafilename))
    return dataout

def subtract(dataobj,args):
    dataout = []
    subtracted = preproc.subtract(dataobj[0].data,dataobj[1].data)
    dataout.append(preproc.datacontainer(subtracted,dataobj[0].dataheader,dataobj[0].datafilename))
    return dataout

def add(dataobj,args):
    dataout = []
    added = preproc.add(dataobj[0].data,dataobj[1].data)
    dataout.append(preproc.datacontainer(added,dataobj[0].dataheader,dataobj[0].datafilename))
    return dataout


if __name__ == '__main__':
    main()